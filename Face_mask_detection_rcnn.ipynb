{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDXrpQl92qbd"
      },
      "source": [
        "Step 1. Import all the necessary libraries for building the object detection model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djS8KlJq2ksk",
        "outputId": "64789ecf-7de1-4de8-aa3a-6bb02462021a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import pycocotools\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "print(torchvision.__version__)\n",
        "!git clone https://github.com/pytorch/vision.git\n",
        "!cp vision/references/detection/utils.py ./\n",
        "!cp vision/references/detection/transforms.py ./\n",
        "!cp vision/references/detection/coco_eval.py ./\n",
        "!cp vision/references/detection/engine.py ./\n",
        "!cp vision/references/detection/coco_utils.py ./\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as TF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i41sA3-RiOmd"
      },
      "source": [
        "Step 2: Download the dataset from the following link. The dataset basically contains images of people wearing face mask, not wearing face mask and wearing the facemask incorrectly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyhlB78p288T",
        "outputId": "e74962ca-e657-483c-e31e-b18dde0fb347"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "url = 'https://empslocal.ex.ac.uk/people/staff/ad735/ECMM426/MaskedFace.zip'  # Replace with your URL\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('dataset.zip', 'wb').write(r.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41BuCyV0igdS"
      },
      "source": [
        "Unzip the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q84KQ3Xd3PgU",
        "outputId": "131e573d-78da-4aca-a06b-7ea53e608384"
      },
      "outputs": [],
      "source": [
        "!unzip dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_VLu_HEikNG"
      },
      "source": [
        "Step 3 : Seperate the annotations files and place it in the seperate directory. Do the same for train and val folders respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQKdROi-3WA1",
        "outputId": "faf5a745-bf16-4a6a-a859-5e05c8c08fc6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def separate_xml_files(source_dir, destination_dir):\n",
        "    # Create the destination directory if it doesn't exist\n",
        "    if not os.path.exists(destination_dir):\n",
        "        os.makedirs(destination_dir)\n",
        "\n",
        "    # List all files in the source directory\n",
        "    files = os.listdir(source_dir)\n",
        "\n",
        "    # Loop through each file\n",
        "    for file_name in files:\n",
        "        # Check if the file is an XML file\n",
        "        if file_name.endswith('.xml'):\n",
        "            # Move XML files to the destination directory\n",
        "            shutil.move(os.path.join(source_dir, file_name), os.path.join(destination_dir, file_name))\n",
        "            print(f\"Moved {file_name} to {destination_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    source_directory = '/content/train'\n",
        "    x = '/content'\n",
        "    destination_directory = os.path.join(x, \"annotations\")\n",
        "\n",
        "    # Call the function to separate XML files\n",
        "    separate_xml_files(source_directory, destination_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdAf2PeA3WzM",
        "outputId": "e557858a-9375-424f-91aa-a478263b272d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def separate_xml_files(source_dir, destination_dir):\n",
        "    # Create the destination directory if it doesn't exist\n",
        "    if not os.path.exists(destination_dir):\n",
        "        os.makedirs(destination_dir)\n",
        "\n",
        "    # List all files in the source directory\n",
        "    files = os.listdir(source_dir)\n",
        "\n",
        "    # Loop through each file\n",
        "    for file_name in files:\n",
        "        # Check if the file is an XML file\n",
        "        if file_name.endswith('.xml'):\n",
        "            # Construct source and destination file paths\n",
        "            source_file = os.path.join(source_dir, file_name)\n",
        "            destination_file = os.path.join(destination_dir, file_name)\n",
        "\n",
        "            # Move XML files to the destination directory\n",
        "            os.rename(source_file, destination_file)\n",
        "            print(f\"Moved {file_name} to {destination_dir}\")\n",
        "\n",
        "source_directory = '/content/val/'\n",
        "destination_directory = os.path.join(source_directory, \"annotations\")\n",
        "separate_xml_files(source_directory, destination_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPgx94dK3e2g"
      },
      "outputs": [],
      "source": [
        "!mv /content/val/annotations /content/sample_data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGh8r9nFi5YA"
      },
      "source": [
        "Step 4: Now create MaskDataset classes for train and val datasets which are objects of these classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFeXiH5h3kcC"
      },
      "outputs": [],
      "source": [
        "## import os\n",
        "from PIL import Image\n",
        "from xml.etree import ElementTree as ET\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "class MaskDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.image_dir = os.path.join(root_dir,'train')  # Correctly store the image directory\n",
        "        self.annot_dir = os.path.join(root_dir, 'annotations')  # Correctly store the annotation directory\n",
        "        self.transform = transform\n",
        "        self.image_names = os.listdir(self.image_dir)\n",
        "        self.image_names.sort()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.image_names[idx]  # Get the image name from the sorted list\n",
        "        image_path = os.path.join(self.image_dir, image_name)  # Construct the correct path to the image file\n",
        "        image = Image.open(image_path).convert(\"RGB\")  # Open the image and convert it to RGB format\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)  # Apply the transformation on the image if transform is not None\n",
        "\n",
        "        target = {}  # Initialize the target dictionary\n",
        "\n",
        "        # Construct the correct path to the annotation file. Assume the annotation filename matches the image name.\n",
        "        # You might need to change the extension or naming convention depending on your dataset.\n",
        "        annot_file_name = image_name.replace('.png', '.xml')  # Change this depending on your file naming convention\n",
        "        annot_path = os.path.join(self.annot_dir, annot_file_name)  # Correct path to the individual annotation file\n",
        "\n",
        "        tree = ET.parse(annot_path)  # Parse the XML annotation file\n",
        "        root = tree.getroot()  # Get the root element of the XML\n",
        "\n",
        "        boxes = []  # List to store the bounding boxes\n",
        "        labels = []  # List to store the corresponding labels\n",
        "\n",
        "        for obj in root.findall('object'):\n",
        "            label = obj.find('name').text  # Get the label of the object\n",
        "\n",
        "            # Map the labels to numeric values\n",
        "            if label == 'with_mask':\n",
        "                labels.append(1)\n",
        "            elif label == 'without_mask':\n",
        "                labels.append(2)\n",
        "            elif label == 'mask_weared_incorrect':\n",
        "                labels.append(3)\n",
        "\n",
        "            bndbox = obj.find('bndbox')  # Get the bounding box coordinates\n",
        "            xmin = int(bndbox.find('xmin').text)\n",
        "            ymin = int(bndbox.find('ymin').text)\n",
        "            xmax = int(bndbox.find('xmax').text)\n",
        "            ymax = int(bndbox.find('ymax').text)\n",
        "\n",
        "            boxes.append([xmin, ymin, xmax, ymax])  # Append the bounding box coordinates\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)  # Convert the bounding boxes to a tensor\n",
        "        labels = torch.tensor(labels)  # Convert the labels to a tensor\n",
        "\n",
        "        target['boxes'] = boxes  # Assign the boxes tensor to 'boxes' key in the target dictionary\n",
        "        target['labels'] = labels  # Assign the labels tensor to 'labels' key in the target dictionary\n",
        "\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPT-1oPf3qDT"
      },
      "outputs": [],
      "source": [
        "## import os\n",
        "from PIL import Image\n",
        "from xml.etree import ElementTree as ET\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "class MaskDataset1(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.image_dir = os.path.join(root_dir,'val')  # Correctly store the image directory\n",
        "        self.annot_dir = os.path.join(root_dir, 'sample_data/annotations')  # Correctly store the annotation directory\n",
        "        self.transform = transform\n",
        "        self.image_names = os.listdir(self.image_dir)\n",
        "        self.image_names.sort()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.image_names[idx]  # Get the image name from the sorted list\n",
        "        image_path = os.path.join(self.image_dir, image_name)  # Construct the correct path to the image file\n",
        "        image = Image.open(image_path).convert(\"RGB\")  # Open the image and convert it to RGB format\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)  # Apply the transformation on the image if transform is not None\n",
        "\n",
        "        target = {}  # Initialize the target dictionary\n",
        "\n",
        "        # Construct the correct path to the annotation file. Assume the annotation filename matches the image name.\n",
        "        # You might need to change the extension or naming convention depending on your dataset.\n",
        "        annot_file_name = image_name.replace('.png', '.xml')  # Change this depending on your file naming convention\n",
        "        annot_path = os.path.join(self.annot_dir, annot_file_name)  # Correct path to the individual annotation file\n",
        "\n",
        "        tree = ET.parse(annot_path)  # Parse the XML annotation file\n",
        "        root = tree.getroot()  # Get the root element of the XML\n",
        "\n",
        "        boxes = []  # List to store the bounding boxes\n",
        "        labels = []  # List to store the corresponding labels\n",
        "\n",
        "        for obj in root.findall('object'):\n",
        "            label = obj.find('name').text  # Get the label of the object\n",
        "\n",
        "            # Map the labels to numeric values\n",
        "            if label == 'with_mask':\n",
        "                labels.append(1)\n",
        "            elif label == 'without_mask':\n",
        "                labels.append(2)\n",
        "            elif label == 'mask_weared_incorrect':\n",
        "                labels.append(3)\n",
        "\n",
        "            bndbox = obj.find('bndbox')  # Get the bounding box coordinates\n",
        "            xmin = int(bndbox.find('xmin').text)\n",
        "            ymin = int(bndbox.find('ymin').text)\n",
        "            xmax = int(bndbox.find('xmax').text)\n",
        "            ymax = int(bndbox.find('ymax').text)\n",
        "\n",
        "            boxes.append([xmin, ymin, xmax, ymax])  # Append the bounding box coordinates\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)  # Convert the bounding boxes to a tensor\n",
        "        labels = torch.tensor(labels)  # Convert the labels to a tensor\n",
        "\n",
        "        target['boxes'] = boxes  # Assign the boxes tensor to 'boxes' key in the target dictionary\n",
        "        target['labels'] = labels  # Assign the labels tensor to 'labels' key in the target dictionary\n",
        "\n",
        "        return image, target\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2GYvA1f3tDy"
      },
      "outputs": [],
      "source": [
        "collate_fn = lambda batch: tuple(zip(*batch))\n",
        "\n",
        "\n",
        "# Define transforms for the dataset\n",
        "train_transforms = transforms.Compose([\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create dataset for training with specified transformations\n",
        "train_dataset = MaskDataset('/content', transform=train_transforms)\n",
        "# Create dataloader for training dataset\n",
        "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIB-CjmY3vyR",
        "outputId": "1610101e-9b2f-4137-9697-f3c871ab30a3"
      },
      "outputs": [],
      "source": [
        "# Fetch a batch of data from the dataloader\n",
        "img, target = next(iter(dataloader))\n",
        "\n",
        "# Print the shape of the first image in the batch\n",
        "print(img[0].shape)\n",
        "\n",
        "# Print the target label of the first image in the batch\n",
        "print(target[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5Zrydamn3j8"
      },
      "source": [
        "Step 5: Now build the faster-rcnn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bNCZmUI3xqe",
        "outputId": "85a3fb76-6365-4be2-f0c8-eb20270bf580"
      },
      "outputs": [],
      "source": [
        "import torchvision # Imports the torchvision library\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor # Imports the Faster R-CNN predictor module from torchvision\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "num_classes = 4 # Define the number of classes in your dataset 3 classes and one background\n",
        "# Define the model\n",
        "def get_model(num_classes):\n",
        "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "  return model\n",
        "model = get_model(num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ9ci366pVqM"
      },
      "source": [
        "Step 6: Define the optimizer as AdamW and also define the learning rate scheduler with step size as 5 and gamma as 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sA-UD3AX5PUa"
      },
      "outputs": [],
      "source": [
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.AdamW(params, lr=1e-4,\n",
        "                              amsgrad=True,\n",
        "                              weight_decay=1e-6)\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=5,\n",
        "                                               gamma=0.5)\n",
        "\n",
        "# Define the device (GPU or CPU)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc0nhitZplfv"
      },
      "source": [
        "Step 7: Define the train_one_epoch function and start training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rgs5fnC55Uke"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n",
        "    model.train()\n",
        "    for batch_idx, (images, targets) in enumerate(data_loader):\n",
        "        # Sending training data to CUDA\n",
        "        images = list([image.to(device) for image in images])\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % print_freq == 0:\n",
        "            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {losses}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khw7mlAo5XBs",
        "outputId": "3ed4e9a3-b012-4986-d581-e55249e311b2"
      },
      "outputs": [],
      "source": [
        "num_epochs = 20\n",
        "\n",
        "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
        "# Train the model\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(model, optimizer, dataloader, device, epoch, print_freq=20)\n",
        "    lr_scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gcTzjW9qCTp"
      },
      "source": [
        "Step 8: Now comes the testing part so define your test loader and set up the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zrDA9EcrcfN",
        "outputId": "db1128df-879d-43a7-bd08-dd0e068b2d71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "85\n"
          ]
        }
      ],
      "source": [
        "import torchvision.ops as ops\n",
        "# Load a single minibatch of data\n",
        "test_dataset = MaskDataset1('/content', transform=train_transforms)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
        "print(len(test_dataset))\n",
        "images, targets = next(iter(test_dataloader))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGv2p87MqMPF"
      },
      "source": [
        "Step 9: Now define the count masks function to print the predicted labels in the form of a Nx3 array which returns the count of people wearing mask properly, not wearing mask and incorrectly wearing mask and also calculate the MAPE score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5EbefLBrsVZ",
        "outputId": "f65787ce-7fdb-4783-d295-871e87705177"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def count_masks(dataset, model, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    all_pred_counts = []  # List to hold all predicted counts for each image\n",
        "    all_true_counts = []  # List to hold all true counts for each image\n",
        "    all_mape_scores = []  # List to hold MAPE scores for each image\n",
        "\n",
        "    with torch.no_grad():  # No need to track gradients\n",
        "        for images, targets in dataset:\n",
        "            images = list(img.to(device) for img in images)\n",
        "            # print(targets)\n",
        "            outputs = model(images)\n",
        "\n",
        "            for i, output in enumerate(outputs):\n",
        "                # Assuming 'labels' from the model output indicate the predicted classes\n",
        "                preds = output['labels'].cpu().numpy()  # Predicted class labels\n",
        "\n",
        "                pred_count_array = [np.sum(preds == class_id) for class_id in range(1, 4)]\n",
        "                all_pred_counts.append(pred_count_array)\n",
        "\n",
        "                # Assuming 'targets' contain the true labels in a similar format\n",
        "                true_labels = targets[i]['labels'].cpu().numpy()\n",
        "                true_count_array = [np.sum(true_labels == label) for label in range(1, 4)]  # Assuming class labels are 1, 2, 3\n",
        "                all_true_counts.append(true_count_array)\n",
        "\n",
        "                # Calculate MAPE for this image and append\n",
        "                mape_scores = np.abs(np.array(pred_count_array) - np.array(true_count_array)) / np.maximum(np.array(true_count_array), 1)\n",
        "                all_mape_scores.append(np.mean(mape_scores) * 100)  # Average MAPE per image, converted to percentage\n",
        "\n",
        "    # Calculate overall average MAPE across all images\n",
        "    average_mape = np.mean(all_mape_scores)\n",
        "\n",
        "    # Convert lists to Numpy arrays\n",
        "    final_pred_counts = np.array(all_pred_counts, dtype=np.int64)\n",
        "    final_true_counts = np.array(all_true_counts, dtype=np.int64)\n",
        "\n",
        "    return final_pred_counts, final_true_counts, average_mape\n",
        "final_pred_counts, final_true_counts, average_mape = count_masks(test_dataloader,model,device)\n",
        "print(final_true_counts,final_pred_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPiD0P8UK4km",
        "outputId": "b28f7bd4-991e-49c0-c22a-542b9447e9f3"
      },
      "outputs": [],
      "source": [
        "print('True Counts [With Mask, Without Mask, Mask Wearing incorrectly] :' , final_true_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lPSGpoLKZqx",
        "outputId": "4aab9c17-2525-4d1b-8019-df67a064d9a3"
      },
      "outputs": [],
      "source": [
        "print('Final Predicted Counts [With Mask, Without Mask, Mask Wearing incorrectly] :' , final_pred_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_XKkKu2rxDw",
        "outputId": "6f5370c2-939b-48d2-bc46-e02599a90c15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAPE SCORE :16.334645771361256 % \n"
          ]
        }
      ],
      "source": [
        "print(f'MAPE SCORE :{average_mape} % ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5untdsKtHDr"
      },
      "source": [
        "Step 10: Display some images of the prediction to see how well the model performed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hcVM9ePzyHIL",
        "outputId": "77318ab1-c602-4cce-8a9e-848ab90ddd00"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "for images, targets in test_dataloader:\n",
        "  with torch.no_grad():\n",
        "      images = list([image.to(device) for image in images])\n",
        "      outputs = model(images)\n",
        "\n",
        "      for i, image in enumerate(images):\n",
        "\n",
        "          boxes = outputs[i]['boxes'].cpu().numpy()\n",
        "\n",
        "          labels = outputs[i]['labels'].cpu().numpy()\n",
        "\n",
        "          # Visualize the input image with the predicted bounding boxes and labels\n",
        "          fig, ax = plt.subplots(1)\n",
        "          ax.imshow(unorm(image).cpu().permute(1, 2, 0))\n",
        "          label_color = ['green', 'red', 'yellow' ]\n",
        "\n",
        "          for box, label in zip(boxes, labels):\n",
        "              x1, y1, x2, y2 = box.astype(int)\n",
        "              ax.add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor=label_color[label - 1], linewidth=2))\n",
        "\n",
        "          plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
